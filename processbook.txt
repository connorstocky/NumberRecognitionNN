Entry 1:
- Set up Github repository
- Researched into dataset and downloaded necessary files.
- Worked on LFS to store the large MNIST data sets
- imported libraries i know i will need
- Created function to convert MNIST data into an array for use by the NN
- Watched videos explaining the dataset and how the labels and pixels are arranged. 
- Researched weightings and biases for layers of the NN.

Entry 2:
- Initialised parameters
- Researched overview of process and thought to start with easy key functions such as activation functions
- Researched into posssible activation functions
- From research there are 3 main ones; Relu, Tanh and Sigmoid. 
- For simplicity i will start by choosing Rectified Linear and then change later to compare if needed
- Researched softmax Function and implemeneted function

Entry 3:
- Adjusted initial parameters function to take an argument of a number of nodes for changes later

Entry 4:
- Researched forward propogation and how it worked
- Implemented a basic forward propogation function
- Fixed a bug in activation functions with incorrect variable naming

Entry 5:
- Researched backwards propogatoin and how to calculate the errors in both the weightings and biases 
- Needed to one_hot encode the solutions so created a function to do that
- Coded a delta activation function as it was needed to solve for errors

Entry 6: 
- With backwards propogation coded, next step is to use the errors to adjust the biases and weightings
- With this in mind i added a function to update the parameters 
- Researched learning rate and implemented into function

Entry 7:
- Lots of debuggins and fixing in this Entry
- Implemented network training and had to adjust other functions to solutions
- Data imported is not in the right format so am getting shape errors and am trying to fix them
- Fixed numerous errors and code is running however there is no change from each iteration 
- After debugging for 2 hours, the issue was the pixels were not normalised and were 255 times as big
- Fixing this resulted in a first working iteration of the training function with accurracy reaching 91%