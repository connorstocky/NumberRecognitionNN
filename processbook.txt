Entry 1:
- Set up Github repository
- Researched into dataset and downloaded necessary files.
- Worked on LFS to store the large MNIST data sets
- imported libraries i know i will need
- Created function to convert MNIST data into an array for use by the NN
- Watched videos explaining the dataset and how the labels and pixels are arranged. 
- Researched weightings and biases for layers of the NN.

Entry 2:
- Initialised parameters
- Researched overview of process and thought to start with easy key functions such as activation functions
- Researched into posssible activation functions
- From research there are 3 main ones; Relu, Tanh and Sigmoid. 
- For simplicity i will start by choosing Rectified Linear and then change later to compare if needed
- Researched softmax Function and implemeneted function

Entry 3:
- Adjusted initial parameters function to take an argument of a number of nodes for changes later

Entry 4:
- Researched forward propogation and how it worked
- Implemented a basic forward propogation function
- Fixed a bug in activation functions with incorrect variable naming